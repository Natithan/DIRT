// Config settings used for SuperGLUE DIRT experiments.

// This imports the defaults, which can be overridden below.
include "defaults.conf"
exp_name = "DIRT"
run_name = "same"
run_dir = ${project_dir}"/"${exp_name}"/"1run_${run_name}
global_ro_exp_dir = ${GLOBAL_RO_EXP_DIR}


// Model settings
input_module = "dirt"
transformers_output_mode = "top"
pair_attn = 0 // shouldn't be needed but JIC
s2s = {
    attention = none
}
sent_enc = "none"
sep_embs_for_skip = 1
classifier = log_reg // following BERT paper
transfer_paradigm = finetune // finetune entire BERT model

// Training settings
dropout = 0.1 // following BERT paper
optimizer = bert_adam
batch_size = 8
max_epochs = 10
lr = .00001
min_lr = .0000001
lr_patience = 4
patience = 20
max_vals = 10000
target_train_val_interval = 500

// Control-flow stuff
do_pretrain = 0
do_target_task_training = 1
do_full_eval = 1
write_preds = "val,test"
write_strict_glue_format = 1

allow_reuse_of_pretraining_parameters = 1

pretrain_tasks = none
//pretrain_tasks = DIR-MLM  // Comma-separated list of pretraining tasks or 'glue' or 'superglue' or 'none'.
                      // If there are multiple entries, the list should contain no spaces, and
                      // should be enclosed in quotes. When using command line overrides, you need
                      // to escape these quotes:
                      //   python main.py --overrides "pretrain_tasks = \"glue,ccg\""
                      // Note: The code expects this to be nonempty in most cases. If you want to
                      // train and evaluate on a single task without doing any new pretraining,
                      // you should set target_tasks and pretraining_tasks to the same task, set
                      // do_pretrain to 1, and do_target_task_training to 0.
target_tasks = superglue
//target_tasks = superglue  // Target tasks, for use in both target_task_training
                     // (if do_target_training = 1) and the final evaluation,
                     // (if do_full_eval = 1), and is in the same list format as pretrain_tasks.